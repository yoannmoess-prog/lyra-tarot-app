/* eslint-env node */
// server/index.js — Serveur de diagnostic avec routes API

import path from "node:path";
import { fileURLToPath } from "node:url";
import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import rateLimit from "express-rate-limit";
import { randomUUID } from "node:crypto";
import { initRag, searchRag, formatRagContext } from "./rag.js";
import OpenAI from "openai";

// --- Configuration initiale ---
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
dotenv.config({ path: path.resolve(__dirname, ".env") });

const app = express();

// --- Constantes et Variables d'environnement ---
const PORT = process.env.PORT || 8787;
const LLM_API_KEY = process.env.LLM_API_KEY || "";
const LLM_MODEL = process.env.LLM_MODEL || "gpt-4o-mini";

const openai = new OpenAI({ apiKey: LLM_API_KEY });

// --- Middlewares principaux ---
app.use(cors({ origin: "*", credentials: false }));
app.use(express.json({ limit: "1mb" }));

// --- Rate Limiter pour l'API ---
app.use(
  "/api/",
  rateLimit({
    windowMs: 10 * 60 * 1000,
    max: 50,
    standardHeaders: true,
    legacyHeaders: false,
  })
);

// --- Prompt Builder ---
function buildMessages({ name, question, cards, userMessage, history }) {
  const systemContent = `
Tu es un thérapeute tarologue expérimenté. Ton rôle est d’interpréter les tirages du Tarot de Marseille en respectant les structures suivantes :
... (contenu du prompt inchangé) ...
  `.trim();
  const safeHistory = Array.isArray(history) ? history.slice(-10) : [];
  const turn = userMessage
    ? [{ role: "user", content: userMessage }]
    : [{
        role: "user",
        content: "C'est mon premier tour après le tirage. Donne-moi ton interprétation complète en suivant la structure demandée."
      }];
  return [{ role: "system", content: systemContent }, ...safeHistory, ...turn];
}

// --- Routes API ---
app.get("/api/healthz", (_, res) => res.json({ ok: true, ts: Date.now() }));

app.post("/api/lyra/stream", async (req, res) => {
  if (!LLM_API_KEY) {
    return res.status(500).json({ error: { code: "missing_api_key", message: "La clé API LLM est absente." } });
  }
  try {
    const { name, question, cards, userMessage, history } = req.body || {};
    const messages = buildMessages({ name, question, cards, userMessage, history });
    const stream = await openai.chat.completions.create({
      model: LLM_MODEL,
      messages: messages,
      stream: true,
      temperature: 0.7,
      top_p: 1,
      max_tokens: 1024,
    });
    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");
    res.flushHeaders();
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || "";
      if (content) {
        res.write(`data: ${JSON.stringify(content)}\n\n`);
      }
    }
    res.end();
  } catch (error) {
    console.error("[lyra] /api/lyra/stream error:", error);
    res.status(500).end("Stream error");
  }
});

// --- Service des fichiers statiques et route "Catch-all" ---
// Doit être placé APRÈS les routes API pour ne pas les intercepter.
const staticPath = path.resolve(__dirname, "..", "dist");
app.use(express.static(staticPath));
app.get("*", (req, res) => {
  res.sendFile(path.resolve(staticPath, "index.html"));
});

// --- Lancement du serveur ---
initRag().catch((e) => console.warn("[rag] init error:", e));
app.listen(PORT, () => {
  console.log(`Lyra backend on http://localhost:${PORT}`);
  console.log(`[lyra] LLM key: ${LLM_API_KEY ? "présente" : "absente"}`);
  console.log(`[lyra] Serving frontend from: ${staticPath}`);
});